{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Problem Statement\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. \n",
    "* You have historical data from previous applicants that you can use as a training set for logistic regression. \n",
    "* For each training example, you have the applicant’s scores on two exams and the admissions decision. \n",
    "* Your task is to build a classification model that estimates an applicant’s probability of admission based on the scores from those two exams. \n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Loading and visualizing the data\n",
    "\n",
    "You will start by loading the dataset for this task. \n",
    "- The `load_dataset()` function shown below loads the data into variables `X_train` and `y_train`\n",
    "  - `X_train` contains exam scores on two exams for a student\n",
    "  - `y_train` is the admission decision \n",
    "      - `y_train = 1` if the student was admitted \n",
    "      - `y_train = 0` if the student was not admitted \n",
    "  - Both `X_train` and `y_train` are numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = [34.62365962, 78.02469282, 30.28671077, 43.89499752, 35.84740877,\n",
    "       72.90219803, 60.18259939, 86.3085521 , 79.03273605, 75.34437644,\n",
    "       45.08327748, 56.31637178, 61.10666454, 96.51142588, 75.02474557,\n",
    "       46.55401354, 76.0987867 , 87.42056972, 84.43281996, 43.53339331,\n",
    "       95.86155507, 38.22527806, 75.01365839, 30.60326323, 82.30705337,\n",
    "       76.4819633 , 69.36458876, 97.71869196, 39.53833914, 76.03681085,\n",
    "       53.97105215, 89.20735014, 69.07014406, 52.74046973, 67.94685548,\n",
    "       46.67857411, 70.66150955, 92.92713789, 76.97878373, 47.57596365,\n",
    "       67.37202755, 42.83843832, 89.67677575, 65.79936593, 50.53478829,\n",
    "       48.85581153, 34.21206098, 44.2095286 , 77.92409145, 68.97235999,\n",
    "       62.27101367, 69.95445795, 80.19018075, 44.82162893, 93.1143888 ,\n",
    "       38.80067034, 61.83020602, 50.25610789, 38.7858038 , 64.99568096,\n",
    "       61.37928945, 72.80788731, 85.40451939, 57.05198398, 52.10797973,\n",
    "       63.12762377, 52.04540477, 69.43286012, 40.23689374, 71.16774802,\n",
    "       54.63510555, 52.21388588, 33.91550011, 98.86943574, 64.17698887,\n",
    "       80.90806059, 74.78925296, 41.57341523, 34.18364003, 75.23772034,\n",
    "       83.90239366, 56.30804622, 51.54772027, 46.85629026, 94.44336777,\n",
    "       65.56892161, 82.36875376, 40.61825516, 51.04775177, 45.82270146,\n",
    "       62.22267576, 52.06099195, 77.19303493, 70.4582    , 97.77159928,\n",
    "       86.72782233, 62.0730638 , 96.76882412, 91.5649745 , 88.69629255,\n",
    "       79.94481794, 74.16311935, 99.27252693, 60.999031  , 90.54671411,\n",
    "       43.39060181, 34.52451385, 60.39634246, 50.28649612, 49.80453881,\n",
    "       49.58667722, 59.80895099, 97.64563396, 68.86157272, 32.57720017,\n",
    "       95.59854761, 74.24869137, 69.82457123, 71.79646206, 78.45356225,\n",
    "       75.39561147, 85.75993667, 35.28611282, 47.02051395, 56.2538175 ,\n",
    "       39.26147251, 30.05882245, 49.59297387, 44.66826172, 66.45008615,\n",
    "       66.56089447, 41.09209808, 40.45755098, 97.53518549, 49.07256322,\n",
    "       51.88321182, 80.27957401, 92.11606081, 66.74671857, 60.99139403,\n",
    "       32.72283304, 43.30717306, 64.03932042, 78.03168802, 72.34649423,\n",
    "       96.22759297, 60.45788574, 73.0949981 , 58.84095622, 75.85844831,\n",
    "       99.8278578 , 72.36925193, 47.26426911, 88.475865  , 50.4581598 ,\n",
    "       75.80985953, 60.45555629, 42.50840944, 82.22666158, 42.71987854,\n",
    "       88.91389642, 69.8037889 , 94.83450672, 45.6943068 , 67.31925747,\n",
    "       66.58935318, 57.23870632, 59.51428198, 80.366756  , 90.9601479 ,\n",
    "       68.46852179, 85.5943071 , 42.07545454, 78.844786  , 75.47770201,\n",
    "       90.424539  , 78.63542435, 96.64742717, 52.34800399, 60.76950526,\n",
    "       94.09433113, 77.15910509, 90.44855097, 87.50879176, 55.48216114,\n",
    "       35.57070347, 74.49269242, 84.84513685, 89.84580671, 45.35828361,\n",
    "       83.48916274, 48.3802858 , 42.26170081, 87.10385094, 99.31500881,\n",
    "       68.77540947, 55.34001756, 64.93193801, 74.775893  , 89.5298129]\n",
    "X_train = np.array(x_train).reshape((100,2))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array([0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
    "    0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
    "    0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
    "    1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
    "    1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
    "    1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five elements in X_train are:\n",
      " [[34.62365962 78.02469282]\n",
      " [30.28671077 43.89499752]\n",
      " [35.84740877 72.90219803]\n",
      " [60.18259939 86.3085521 ]\n",
      " [79.03273605 75.34437644]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First five elements in X_train are:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five elements in y_train are:\n",
      " [0. 0. 0. 1. 1.]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First five elements in y_train are:\\n\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (100, 2)\n",
      "The shape of y_train is: (100,)\n",
      "We have m = 100 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m) :\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += - y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)\n",
    "    total_cost = cost / m    \n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at test w,b: 0.218\n"
     ]
    }
   ],
   "source": [
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Cost at test w,b: {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b) :\n",
    "    \n",
    "    m,n = x.shape\n",
    "    w_d = np.zeros(w.shape)\n",
    "    b_d = 0.\n",
    "    \n",
    "    for i in range(m) :\n",
    "        \n",
    "        f_wb = sigmoid(np.dot(x[i],w) + b)\n",
    "        err = f_wb - y[i]\n",
    "        \n",
    "        for j in range(n) :\n",
    "        \n",
    "            w_d = w_d + err *x[i,j]\n",
    "        \n",
    "        b_d = b_d + err\n",
    "        \n",
    "    w_d = w_d / m\n",
    "    b_d = b_d / m\n",
    "    \n",
    "    return w_d , b_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw at initial w (zeros):[-23.27205879 -23.27205879]\n",
      "dj_db at initial b (zeros):-0.1\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_dw at initial w (zeros):{dj_db}' )\n",
    "print(f'dj_db at initial b (zeros):{dj_dw.tolist()}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at test_w: [-89.20519487 -89.20519487]\n",
      "dj_dw at test_w: -0.5999999999991071\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost and gradient with non-zero w\n",
    "test_w = np.array([ 0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db at test_w:', dj_db)\n",
    "print('dj_dw at test_w:', dj_dw.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, compute_cost, compute_gradient, alpha, num_iters, lambda_): \n",
    "    \n",
    "    m = len(x)\n",
    "    \n",
    "    j_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters) :\n",
    "        \n",
    "        w_d , b_d = compute_gradient(x,y,w_in,b_in)\n",
    "        \n",
    "        w_in = w_in - alpha * w_d \n",
    "        b_in = b_in - alpha * b_d \n",
    "        \n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  compute_cost(x, y, w_in, b_in)\n",
    "            j_history.append(cost)\n",
    "        \n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(j_history[-1]):8.2f}   \")        \n",
    "            \n",
    "    return w_in , b_in ,j_history ,w_history            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.75   \n",
      "Iteration 1000: Cost     0.65   \n",
      "Iteration 2000: Cost     0.65   \n",
      "Iteration 3000: Cost     0.64   \n",
      "Iteration 4000: Cost     0.64   \n",
      "Iteration 5000: Cost     0.63   \n",
      "Iteration 6000: Cost     0.63   \n",
      "Iteration 7000: Cost     0.62   \n",
      "Iteration 8000: Cost     0.62   \n",
      "Iteration 9000: Cost     0.61   \n",
      "Iteration 9999: Cost     0.44   \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "intial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "# iter = []\n",
    "# for i in range(iterations) :\n",
    "#     iter.append(i)\n",
    "w,b, J_history,w_history = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                            compute_cost, compute_gradient, alpha, iterations, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w,b) :\n",
    "    \n",
    "    m,n = x.shape\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    for i in range(m) :\n",
    "        \n",
    "        z = np.dot (x[i] , w) \n",
    "        \n",
    "        for j in range(n) :\n",
    "            \n",
    "            z += 0\n",
    "        z += b\n",
    "            \n",
    "        f_wb = sigmoid(z)\n",
    "    \n",
    "        if (f_wb >= 0.5) :\n",
    "            p[i] = 1\n",
    "        else :\n",
    "            p[i] = 0\n",
    "    \n",
    "    return p            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.02817175 -1.57296862]\n",
      " [ 0.36540763 -2.8015387 ]\n",
      " [ 1.24481176 -1.2612069 ]\n",
      " [-0.1809609  -0.74937038]]\n",
      "Output of predict: shape (4,), value [0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Test your predict code\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "print(tmp_X)\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7608d97510eb413e289a9ba9d2d6d39358a0b182af5f310d2c58353f8e6821a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
